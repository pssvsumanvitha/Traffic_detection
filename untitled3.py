# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ASKuQ79LPKaCZUOFWlQ3ptuapC5jvKjh
"""



from pyspark.sql import SparkSession
from pyspark.sql.functions import hour, dayofweek, col, when
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

spark = SparkSession.builder.appName("Traffic Intrusion Detection").getOrCreate()

df = spark.read.csv("/content/traffic.csv.csv", header=True, inferSchema=True)

df.printSchema()
df.show(5)

df = df.withColumn("Hour", hour(col("DateTime"))) \
       .withColumn("DayOfWeek", dayofweek(col("DateTime")))

df = df.withColumn("Intrusion", when(col("Vehicles") > 20, 1).otherwise(0))

feature_cols = ["Vehicles", "Junction", "Hour", "DayOfWeek"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

rf = RandomForestClassifier(labelCol="Intrusion", featuresCol="features", numTrees=50)
pipeline = Pipeline(stages=[assembler, rf])

train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)
model = pipeline.fit(train_data)
predictions = model.transform(test_data)

evaluator_auc = BinaryClassificationEvaluator(labelCol="Intrusion", metricName="areaUnderROC")
auc = evaluator_auc.evaluate(predictions)
print(f"AUC Score: {auc:.4f}")

evaluator_acc = MulticlassClassificationEvaluator(labelCol="Intrusion", predictionCol="prediction", metricName="accuracy")
evaluator_precision = MulticlassClassificationEvaluator(labelCol="Intrusion", predictionCol="prediction", metricName="weightedPrecision")
evaluator_recall = MulticlassClassificationEvaluator(labelCol="Intrusion", predictionCol="prediction", metricName="weightedRecall")
evaluator_f1 = MulticlassClassificationEvaluator(labelCol="Intrusion", predictionCol="prediction", metricName="f1")

accuracy = evaluator_acc.evaluate(predictions)
precision = evaluator_precision.evaluate(predictions)
recall = evaluator_recall.evaluate(predictions)
f1 = evaluator_f1.evaluate(predictions)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

predictions.select("Vehicles", "Junction", "Hour", "DayOfWeek", "Intrusion", "prediction").show(10)

spark.stop()